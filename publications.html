<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Publications </title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="publications.html" class="current">Publications</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Publications </h1>
</div>
<p>* alphabetical order or equal contributions</p>

<h2>Preprints </h2>
<p><a href="https://arxiv.org/pdf/2202.00275.pdf"><b>Architecture Matters in Continual Learning</b></a><br>
Seyed Iman Mirzadeh, Arslan Chaudhry, Dong Yin, Timothy Nguyen, Razvan Pascanu, Dilan Gorur, Mehrdad Farajtabar<br /></p>

<p><a href="https://arxiv.org/pdf/2110.11526.pdf"><b>Wide Neural Networks Forget Less Catastrophically</b></a><br>
Seyed Iman Mirzadeh, Arslan Chaudhry, Dong Yin, Huiyi Hu, Razvan Pascanu, Dilan Gorur, Mehrdad Farajtabar<br /></p>

<p><a href="https://arxiv.org/pdf/2107.11413.pdf"><b>An Instance-Dependent Simulation Framework for Learning with Label Noise</b></a>  <br>
Keren Gu, Xander Masotto, Vandana Bachani, Balaji Lakshminarayanan, Jack Nikodem, Dong Yin<br /></p>

<h2>Journal and Conference Papers </h2>
<p><a href="https://arxiv.org/pdf/2108.05533.pdf"><b>Efficient Local Planning with Linear Function Approximation</b></a>  <br>
Dong Yin, Botao Hao, Yasin Abbasi-Yadkori, Nevena Lazic, Csaba Szepesvari<br />
International Conference on Algorithmic Learning Theory (ALT), 2022.</p>

<p><b>Confident Least Square Value Iteration with Local Access to a Simulator</b> <br>
Botao Hao, Nevena Lazic, Dong Yin, Yasin Abbasi-Yadkori, Csaba Szepesvari<br />
International Conference on Artificial Intelligence and Statistics (AISTATS), 2022.</p>

<p><a href="https://arxiv.org/pdf/2102.12611v1.pdf"><b>Improved Regret Bound and Experience Replay in Regularized Policy Iteration</b></a>  <br>
Nevena Lazic, Dong Yin, ‪Yasin Abbasi-Yadkori‬, ‪Csaba Szepesvari<br />
International Conference on Machine Learning (ICML), 2021 (long talk).</p>

<p><a href="https://arxiv.org/pdf/2006.04088.pdf"><b>An Efficient Framework for Clustered Federated Learning</b></a>  <br>
Avishek Ghosh*, Jichan Chung*, Dong Yin*, Kannan Ramchandran <br />
Annual Conference on Neural Information Processing Systems (NeurIPS), 2020. <br />
Shorter version at ICML Workshop on Federated Learning for User Privacy and Data Confidentiality, 2020.</p>

<p><a href="https://arxiv.org/pdf/2006.12620.pdf"><b>A Maximum-Entropy Approach to Off-Policy Evaluation in Average-Reward MDPs</b></a>  <br>
Nevena Lazic, Dong Yin, Mehrdad Farajtabar, Nir Levine, Dilan Gorur, Chris Harris, Dale Schuurmans <br />
Annual Conference on Neural Information Processing Systems (NeurIPS), 2020. <br /></p>

<p><a href="https://arxiv.org/pdf/1907.03215.pdf"><b>Stochastic Gradient and Langevin Processes</b></a> <br />
Xiang Cheng, Dong Yin, Peter Bartlett, Michael Jordan  <br />
International Conference on Machine Learning (ICML), 2020.</p>

<p><a href="https://arxiv.org/pdf/1906.08988.pdf"><b>A Fourier Perspective on Model Robustness in Computer Vision</b></a>  <br>
Dong Yin, Raphael Gontijo Lopes, Jonathon Shlens, Ekin D. Cubuk, Justin Gilmer  <br />
Annual Conference on Neural Information Processing Systems (NeurIPS), 2019. <br />
Shorter version at ICML Workshop on Uncertainty and Robustness in Deep Learning, 2019. </p>

<p><a href="https://arxiv.org/pdf/1810.11914.pdf"><b>Rademacher Complexity for Adversarially Robust Generalization</b></a> <br />
Dong Yin, Kannan Ramchandran, Peter Bartlett  <br />
International Conference on Machine Learning (ICML), 2019.</p>

<p><a href="https://arxiv.org/pdf/1806.05358.pdf"><b>Defending Against Saddle Point Attack in Byzantine-Robust Distributed Learning</b></a> <br />
Dong Yin, Yudong Chen, Kannan Ramchandran, Peter Bartlett   <br />
International Conference on Machine Learning (ICML), 2019 (long talk).  <br />
Shorter version at ICML Nonconvex Optimization Workshop, 2018. </p>

<p><a href="https://arxiv.org/pdf/1412.7646.pdf"><b>Sub-linear Time Support Recovery for Compressed Sensing using Sparse-Graph Codes</b></a>  <br />
Xiao Li, Dong Yin, Sameer Pawar, Ramtin Pedarsani, Kannan Ramchandran  <br />
IEEE Transactions on Information Theory, October 2019.</p>

<p><a href="https://arxiv.org/pdf/1703.00641.pdf"><b>Learning Mixtures of Sparse Linear Regressions Using Sparse Graph Codes</b></a> <br />
Dong Yin, Ramtin Pedarsani, Yudong Chen, Kannan Ramchandran <br />
IEEE Transactions on Information Theory, March 2019.</p>

<p><a href="https://arxiv.org/pdf/1802.05315.pdf"><b>Online Learning for Non-Stationary A/B Tests</b></a> <br />
Andres Munoz Medina*, Sergei Vassilvitskii*, Dong Yin*  <br />
ACM International Conference on Information and Knowledge Management (CIKM), 2018.</p>

<p><a href="https://arxiv.org/pdf/1803.01498.pdf"><b>Byzantine-Robust Distributed Learning: Towards Optimal Statistical Rates</b></a>  <br />
Dong Yin, Yudong Chen, Kannan Ramchandran, Peter Bartlett   <br />
International Conference on Machine Learning (ICML), 2018 (long talk).</p>

<p><a href="https://arxiv.org/pdf/1706.05699.pdf"><b>Gradient Diversity: a Key Ingredient for Scalable Distributed Learning</b></a> <br />
Dong Yin, Ashwin Pananjady, Max Lam, Dimitris Papailiopoulos, Kannan Ramchandran, Peter Bartlett <br />
International Conference on Artificial Intelligence and Statistics (AISTATS), 2018. <br />
Shorter version at NIPS Optimization Workshop, 2017 (oral presentation).</p>

<h2>Workshop Papers </h2>
<p><a href="https://offline-rl-neurips.github.io/2021/pdf/17.pdf"><b>Importance of Representation Learning for Off-Policy Fitted Q-Evaluation</b></a>  <br>
Xian Wu, Nevena Lazic, Dong Yin, Cosmin Paduraru<br />
NeurIPS Offline Reinforcement Learning Workshop, 2021.

<p><a href="https://drive.google.com/file/d/1zL7_7_PaCeUpDlxv-Dph3LvpPRQRWVLL/view"><b>Revisiting Memory Replay For Large Scale Continual Learning</b></a>  <br>
Yogesh Balaji, Mehrdad Farajtabar, Dong Yin, Alex Mott, Ang Li<br />
Workshop on Continual Learning in Computer Vision, 2021. <a href="https://arxiv.org/pdf/2010.02418.pdf">arXiv</a></p>

<p><a href="https://arxiv.org/pdf/2006.10974.pdf"><b>Optimization and Generalization of Regularization-Based Continual Learning: a Loss Approximation Viewpoint</b></a>  <br>
Dong Yin, Mehrdad Farajtabar, Ang Li, Nir Levine, Alex Mott<br />
<a href="https://drive.google.com/file/d/18KxC84XU3hXwHpBf9coLI7Ajkt5EdaHK/view">Preliminary version with a different title</a> presented at ICML Workshop on Continual Learning, 2020 (spotlight). </p>

<p><a href="https://arxiv.org/pdf/1906.02611.pdf"><b>Improving Robustness Without Sacrificing Accuracy with Patch Gaussian Augmentation</b></a>  <br>
Raphael Gontijo Lopes, Dong Yin, Ben Poole, Justin Gilmer, Ekin D. Cubuk  <br>
ICML Workshop on Uncertainty and Robustness in Deep Learning, 2019. </p>

<p><a href="https://arxiv.org/pdf/1906.06629.pdf"><b>Robust Federated Learning in a Heterogeneous Environment</b></a>  <br>
Avishek Ghosh, Justin Hong, Dong Yin, Kannan Ramchandran  <br>
ICML Workshop on Security and Privacy of Machine Learning, 2019. </p>

<h2>Earlier Papers </h2>

<p><a href="https://arxiv.org/pdf/1703.00641.pdf"><b>Learning Mixtures of Sparse Linear Regressions Using Sparse Graph Codes</b></a> <br />
Dong Yin, Ramtin Pedarsani, Yudong Chen, Kannan Ramchandran <br />
IEEE Allerton Conference on Communication, Control, and Computing, 2017.</p>

<p><a href="https://arxiv.org/pdf/1408.0034.pdf"><b>PhaseCode: Fast and Efficient Compressive Phase Retrieval based on Sparse-Graph Codes</b></a> <br />
Ramtin Pedarsani, Dong Yin, Kangwook Lee, Kannan Ramchandran <br />
IEEE Transactions on Information Theory, April 2017. </p>

<p><a href="http://www.jmlr.org/papers/volume18/16-270/16-270.pdf"><b>Distributed Sequence Memory of Multidimensional Inputs in Recurrent Networks</b></a> <br />
Adam S. Charles, Dong Yin, Christopher J. Rozell <br />
Journal of Machine Learning Research (JMLR), Janurary 2017. </p>

<p><a href="Continuous.pdf"><b>Compressed Sensing Using Sparse-Graph Codes for the Continuous-Alphabet Setting</b></a> <br />
Dong Yin, Ramtin Pedarsani, Xiao Li, Kannan Ramchandran <br />
IEEE Allerton Conference on Communication, Control, and Computing, 2016.</p>

<p><a href="https://arxiv.org/pdf/1606.00531.pdf"><b>Fast and Robust Compressive Phase Retrieval with Sparse-graph Codes</b></a> <br />
Dong Yin, Kangwook Lee, Ramtin Pedarsani, Kannan Ramchandran <br />
IEEE International Symposium on Information Theory (ISIT), 2015.</p>

<p><a href="stm.pdf"><b>Can Random Linear Networks Store Multiple Long Input Streams?</b></a> <br />
Adam S. Charles, Dong Yin, Christopher J. Rozell <br />
IEEE Global Conference on Signal and Information Processing (GlobalSIP), 2014.</p>

<p><a href="affine_projection.pdf"><b>Sparse Constraint Affine Projection Algorithm with Parallel Implementation and Application in Compressive Sensing</b></a> <br />
Dong Yin, Hing Cheung So, Yuantao Gu <br />
IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2014.</p>


<div id="footer">
<div id="footer-text">
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
